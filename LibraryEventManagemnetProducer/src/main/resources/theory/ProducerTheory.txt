*********************************************************************************************
what is record?

->  Records can reduce the boilerplate code that is typically required for creating data carrier classes,
    such as POJOs (Plain Old Java Objects) and DTOs (Data Transfer Objects).

->  As of JDK 14, we can replace our repetitious data classes with records. Records are immutable data classes
    that require only the type and name of fields. The equals, hashCode, and toString methods, as well as the private,
    final fields and public constructor, are generated by the Java compiler

->  we can have inside body ({}) static method and instance method id =f we need ny just like normal class.

*********************************************************************************************
When we add below properties:
     admin:
          properties:
            bootstrap.servers: localhost:9092,localhost:9093,localhost:9094
-> Ideally in Kafka , Topic details like total number of partition, replica etc are configured by Kafka admin team,
    If developer has to do configuration of these details, we have to create a  NewTopic bean and it will
    fetch all the details from admin: properties section.

*********************************************************************************************
Round Robbin Algorithm:
    -> If we have 4 partitions inside a topic, the data will get inserted into the partitions
    with round robbin technique.
    1st data -> part0
    3rd data -> part2
    5th data -> part1
*********************************************************************************************
Why to have more partitions and brokers?
-> To make it efficient, the more partitions we have, more parallelism can be achieved using
    consumer groups. 4 partitions 4 instances of same consumer group

*********************************************************************************************
How to send message to dedicated partitions?
    with key value , we can send to dedicated partioner and also we can implement custom partitioner logic
     -> int partition_number = counter.getAndIncrement() % PARTITION_COUNT;
        kafkaTemplate.send(TOPIC_NAME, partition_number, null, message);

****************************Producer Failure Retry Attempts*****************************************************************
What will happen if producer is up and running and broker gets down?
    -> If the broken suddenly gets down and we want to insert record to topic we will keep one getting
        below error message until the broker gets up:
            Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
        and once the all servers/brokers is up and running, It will start working as expected.

what is acks?
    acks= -1: (all)
        guarantees the messages will be written to leader and all the replicas
    acks= 1:
            guarantees the messages will be written to leader
    ack=0:
            No guarantees

Retry Configurations:
    retries-
        The retries setting determines how many times the producer will attempt to send a message before
        marking it as failed. The default values are:
        -> 0 for Kafka <= 2.0
        -> MAX_INT, i.e., 2147483647 for Kafka >= 2.1

    delivery.timeout.ms-
        If retries > 0, for example, retries = 2147483647, the producer won’t try the request forever,
        it’s bounded by a timeout
        -> delivery.timeout.ms=120000 (= 2 minutes default ). Records will be failed if they can’t be
        delivered in delivery.timeout.ms

    retry.backoff.ms-
        By default, the producer will wait 100ms between retries, but you can control this using the
        retry.backoff.ms parameter

        // Config number of retries
        ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, 5000,
        ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 200,
        ProducerConfig.RETRY_BACKOFF_MS_CONFIG, 500

*********************************************************************************************
sendLibraryEventApproach1():
    This approach uses async call to insert data into topic, So it won't wait to insert data into topic
    and will move on.
    We can confirm by log:
        First below log printed
            -> LibraryEventController:publishBook-Producer successfully written message to topic"
        then handleSuccess() executes.
            -> Message {} successfully written to the kafka topic {} in partition {}

*********************************************************************************************
sendLibraryEventApproach2():
    This is blocking call, Until the message publish to the topic, the control will wait for 5 second and
    even after waiting for 5 sec, messages  doesn't gets write to topic, It will throw time out exception.
    Log order:
        LibraryEventProducerAsync:sendLibraryEventApproach2- message successfully written to topic
        LibraryEventController:publishBook-Producer successfully written message to topic

*********************************************************************************************
sendLibraryEventApproach3():
    This approach uses async call as well to insert data into topic, Here we use producerRecord while
    inserting data to.

    Here we can pass more information inside header along with message to the topic.

*********************************************************************************************










